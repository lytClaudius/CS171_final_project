\documentclass[acmtog]{acmart}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{natbib}
\usepackage{listings}
\usepackage{bm}

\definecolor{blve}{rgb}{0.3372549 , 0.61176471, 0.83921569}
\definecolor{gr33n}{rgb}{0.29019608, 0.7372549, 0.64705882}
\makeatletter
\lst@InstallKeywords k{class}{classstyle}\slshape{classstyle}{}ld
\makeatother
\lstset{language=C++,
	breaklines=true,
	basicstyle=\ttfamily,
	keywordstyle=\color{blve}\ttfamily,
	stringstyle=\color{red}\ttfamily,
	commentstyle=\color{magenta}\ttfamily,
	morecomment=[l][\color{magenta}]{\#},
	classstyle = \bfseries\color{gr33n}, 
	tabsize=2
}
\lstset{basicstyle=\ttfamily}

% Title portion
\title{Assignment 2:\\ {Geometric Modeling}} 

\author{Name:\quad Li Yuetong  \\ student number:\ 2023533126
\\email:\quad liyt2023@shanghaitech.edu.cn}

% Document starts
\begin{document}
\maketitle

\vspace*{2 ex}

\section{Introduction}
\quad In the field of computer graphics, geometric modeling serves as the fundamental basis for creating
and representing three-dimensional objects. The ability to construct and render complex geometric shapes
accurately and efficiently is crucial for a wide range of applications, including game development, visual
effects, industrial design, and scientific visualization. This assignment is designed to explore the core
techniques of geometric modeling, progressing from fundamental mesh construction to advanced surface subdivision
and generation.\\
\quad This report will provide a detailed account of the implementation process for a series of tasks.
We begin with the most basic polygonal mesh, constructing a cube defined by a quadrilateral mesh and converting
it to the more common triangle mesh for rendering. Building on this foundation, we will implement the classic
Catmull-Clark subdivision algorithm, investigating how to iteratively smooth a coarse polygonal mesh to generate
more organic and refined surfaces. To enhance visual realism, we will also implement the Phong lighting model.
Furthermore, to improve user experience and application performance, we will develop an interactive UI to
dynamically adjust the subdivision level and implement an adaptive Level of Detail (LOD) system based on camera
distance. Finally, we will explore the generation of parametric surfaces by creating a Bézier surface from a
given set of control points and integrating it with the Catmull-Clark algorithm, adapting it to correctly handle
the boundary conditions of open surfaces.\\
\quad This report will follow the order of implementation, detailing the underlying principles, implementation
details, key code snippets, and final visual outcomes for each feature, accompanied by an analysis of the results.

\section{Implementation Details}
\subsection{Cube Construction}
\quad Our first task is to construct and render a cube in quadrilateral mesh format. The task requires us that
we should only store the cube in quadrilateral mesh format. So we should firstly define the 8 vertices of the
cube and then define the 6 faces of the cube using these vertices. Each face is defined by 4 vertices,
forming a quadrilateral. Here we need to pay attention to the order of the vertices when defining each face.
Because we need to compute normals for lighting calculations later, we should ensure that the order of the vertices
is counter-clockwise when viewed from the outside of the cube. This way, the computed normals will point outward
from the surface of the cube. In my implementation, I defined a simple cube:
\begin{lstlisting}[language=C++]
std::vector<Vertex> cube_vertices = {
    {{-1.0f, -1.0f, -1.0f}},
    {{1.0f, -1.0f, -1.0f}},
    {{1.0f, 1.0f, -1.0f}},
    {{-1.0f, 1.0f, -1.0f}},
    {{-1.0f, -1.0f, 1.0f}},
    {{1.0f, -1.0f, 1.0f}},
    {{1.0f, 1.0f, 1.0f}},
    {{-1.0f, 1.0f, 1.0f}}};

std::vector<GLuint> quad_indices = {
    0, 3, 2, 1,
    4, 5, 6, 7,
    0, 1, 5, 4,
    2, 3, 7, 6,
    0, 4, 7, 3,
    1, 2, 6, 5};
\end{lstlisting}
\quad After defining the basic data of the cube, we need to pass and store these data in
the \verb|Object| class for drawing next step. I modified the init function to accept
the vertices and indices as parameters, and then initialize VAO,VBO,EBO and related variables.
Consider the task later, I put the binding part of VAO,VBO,EBO in a separate function
called \verb|UpdateBufferData|, which can be called whenever we need to update the data to GPU.
\begin{lstlisting}[language=C++]
void Object::init(const std::vector<Vertex> &verts, const std::vector<GLuint> &inds, GLenum mode){
		glGenVertexArrays(1, &VAO);
		glGenBuffers(1, &VBO);
		glGenBuffers(1, &EBO);
		vertices = verts;
		indices = inds;

		draw_mode.primitive_mode = mode;
	}
void Object::UpdateBufferData(){
		glBindVertexArray(VAO);

		glBindBuffer(GL_ARRAY_BUFFER, VBO);
		glBufferData(GL_ARRAY_BUFFER, vertices.size() * sizeof(Vertex),
		vertices.data(), GL_STATIC_DRAW);

		glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO);
		glBufferData(GL_ELEMENT_ARRAY_BUFFER, indices.size() * sizeof(GLuint),
		indices.data(), GL_STATIC_DRAW);

		// Vertex positions
		glEnableVertexAttribArray(0);
		glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, sizeof(Vertex),
		(void *)offsetof(Vertex, position));

		// Vertex normals
		glEnableVertexAttribArray(1);
		glVertexAttribPointer(1, 3, GL_FLOAT, GL_FALSE, sizeof(Vertex),
		(void *)offsetof(Vertex, normal));

		glBindVertexArray(0);
	}
\end{lstlisting}
\quad For drawing the cube, we just need to bind the VAO and call \verb|glDrawElements| with the appropriate
parameters. If we want to use a shader, we just need to active the shader using \verb|shader.use()| and pass
the necessary uniform variables to the shader before drawing. For debuging convenience, I choose to pass thoes
uniform variables in the main loop before drawing the object.
\begin{lstlisting}
void Object::drawElements(const Shader &shader) const{
    shader.use();
    glBindVertexArray(VAO);
    glDrawElements(draw_mode.primitive_mode, static_cast<GLsizei>(indices.size()), GL_UNSIGNED_INT, 0);
    glBindVertexArray(0);
}
\end{lstlisting}
\quad Because we usually use triangles to render objects in OpenGL, I also implemented a function
to convert quadrilateral faces to triangular faces. This function takes the quadrilateral indices as input
and generates a new set of indices representing the triangular faces. Each quadrilateral face is split into
two triangles by connecting one of its diagonals also in counter-clockwise order.
\begin{lstlisting}
std::vector<GLuint> Object::quad_to_triangles(const std::vector<GLuint> &quad_indices){
    std::vector<GLuint> triangle_indices;
    for (size_t i = 0; i < quad_indices.size(); i += 4){
        GLuint v0 = quad_indices[i];
        GLuint v1 = quad_indices[i + 1];
        GLuint v2 = quad_indices[i + 2];
        GLuint v3 = quad_indices[i + 3];

        // First triangle
        triangle_indices.push_back(v0);
        triangle_indices.push_back(v1);
        triangle_indices.push_back(v2);

        // Second triangle
        triangle_indices.push_back(v2);
        triangle_indices.push_back(v3);
        triangle_indices.push_back(v0);
    }
    return triangle_indices;
}
\end{lstlisting}
\quad In the main function, after creating the cube object, I call the modified init function to initialize the cube with
the defined vertices and indices. Then I transfer the cube to triangles, update the buffer data to GPU, and finally draw the cube in the render loop.
The final effect is shown in Figure \ref{1}.
(For a better view, I implement a simple fixed lighting program here.)
\begin{lstlisting}
int main(){
	//...
	Object cube;
	cube.init(cube_vertices, quad_indices, GL_TRIANGLES);
    glShadeModel(GL_SMOOTH);
    glEnable(GL_LIGHTING);
    glEnable(GL_LIGHT0);
	// render the cube in wireframe mode
	glPolygonMode(GL_FRONT_AND_BACK, GL_LINE);

	// In the render loop
	//...
	glMatrixMode(GL_PROJECTION);
	glLoadIdentity();

	glFrustum(-1.0f, 1.0f, -1.0f * HEIGHT / WIDTH, 1.0f * HEIGHT / WIDTH, 1.0f, 100.0f);

	glMatrixMode(GL_MODELVIEW);
	glLoadIdentity();

	glm::mat4 view = mycamera.getViewMatrix();
	glLoadMatrixf(&view[0][0]);

	GLfloat position[] = {5.0f, 5.0f, 2.0f, 1.0f};
	glLightfv(GL_LIGHT0, GL_POSITION, position);
	cube.drawElements();
	//...
}
\end{lstlisting}
\begin{figure}[t]
	\centering
	\subfigure[triangle mesh]{
		\label{Fig.Cube_render.1}
		\includegraphics[width=0.2\textwidth]{cube_render_triangle.png}
	}
	\subfigure[quadrilateral mesh]{
		\label{Fig.Cube_render.2}
		\includegraphics[width=0.2\textwidth]{cube_render_quadric.png}
	}
	\caption{Cube rendered in triangle mesh and quadrilateral mesh respectively.}
	\label{1}
\end{figure}

\subsection{Catmull-Clark Subdivision}
\quad The Catmull-Clark subdivision algorithm is a widely used technique in computer graphics
for generating smooth surfaces from coarse polygonal meshes, particularly quadrilateral meshes.
The algorithm works by refining the mesh through a series of steps that create new vertices and faces,
resulting in a smoother appearance with each iteration. The main steps of the Catmull-Clark subdivision
algorithm are as follows:
\begin{itemize}
	\item Face Points: For each face in the original mesh, compute a face point by averaging the positions of all vertices that make up the face.
	\item Edge Points: For each edge in the original mesh, compute an edge point by averaging the positions of the two vertices that define the edge and the two face points of the faces adjacent to that edge.
	\item Vertex Points: For each vertex in the original mesh, compute a new vertex point using a weighted average of the original vertex position, the average of the face points of all faces adjacent to the vertex, and the average of the midpoints of all edges connected to the vertex. The formula for computing the new vertex point is:
	      \[
		      V' = \frac{F + 2R + (n-3)S}{n}
	      \]
	      where \(V'\) is the new vertex position, \(F\) is the average of adjacent face points,
	      \(R\) is the average of midpoints of adjacent edges, \(S\) is the original vertex position,
	      and \(n\) is the degree of the vertex (the number of adjacent vertices).
	\item Create New Faces: Construct new faces using the newly computed face points, edge points, and vertex points.
	      Each original face is replaced by a set of smaller faces formed by connecting these new points.
\end{itemize}
First of all, we need to figure out the topology of the mesh, including the adjacency information of faces, edges, and vertices.
In my implementation, I used several data structures to store this information:
\begin{lstlisting}
// this is a simple helper function to get a unique key for an edge by always ordering the vertex indices in ascending order.
std::pair<GLuint, GLuint> getEdgeKey(GLuint v1, GLuint v2){
    if (v1 < v2)
        return {v1, v2};
    else
        return {v2, v1};
}

void Object::CatmullClarkSubdivision(int level){
	// Step 1: Build topology information
	// construct a unique set of edges:
	std::set<std::pair<GLuint, GLuint>> edges_set;
	// store these unique edges in a vector for a clearer order:
	std::vector<std::pair<GLuint, GLuint>> edges;
	// faces adjacent to each vertex:
	std::vector<std::vector<GLuint>> vertex_faces(vertices.size());      
	// faces adjacent to each edge:
	std::map<std::pair<GLuint, GLuint>, std::vector<GLuint>> edge_faces; 
	// edges adjacent to each face:
	std::map<GLuint, std::vector<std::pair<GLuint, GLuint>>> face_edges; 

	for (int i = 0; i < indices.size() / 4; ++i){
		GLuint v0 = indices[4 * i];
		GLuint v1 = indices[4 * i + 1];
		GLuint v2 = indices[4 * i + 2];
		GLuint v3 = indices[4 * i + 3];

		// Record faces adjacent to vertices
		vertex_faces[v0].push_back(i);
		vertex_faces[v1].push_back(i);
		vertex_faces[v2].push_back(i);
		vertex_faces[v3].push_back(i);

		// Record faces adjacent to edges
		edge_faces[getEdgeKey(v0,v1)].push_back(i);
		edge_faces[getEdgeKey(v1,v2)].push_back(i);
		edge_faces[getEdgeKey(v2,v3)].push_back(i);
		edge_faces[getEdgeKey(v3,v0)].push_back(i);

		// Record edges adjacent to faces
		face_edges[i].push_back(getEdgeKey(v0, v1));
		face_edges[i].push_back(getEdgeKey(v1, v2));
		face_edges[i].push_back(getEdgeKey(v2, v3));
		face_edges[i].push_back(getEdgeKey(v3, v0));

		// Collect unique edges
		edges_set.insert(getEdgeKey(v0, v1));
		edges_set.insert(getEdgeKey(v1, v2));
		edges_set.insert(getEdgeKey(v2, v3));
		edges_set.insert(getEdgeKey(v3, v0));
	}
	edges.assign(edges_set.begin(), edges_set.end());
	// ... continue with the rest of the Catmull-Clark subdivision steps
}
\end{lstlisting}
\quad After building the topology information, we can proceed to compute the new face points, edge points, and vertex points according to the Catmull-Clark subdivision rules.
\begin{lstlisting}
// Step 2: Compute new points
// face points
std::vector<Vertex> face_points;
for (int i = 0; i < indices.size() / 4; ++i){
	GLuint v0 = indices[4 * i];
	GLuint v1 = indices[4 * i + 1];
	GLuint v2 = indices[4 * i + 2];
	GLuint v3 = indices[4 * i + 3];
	vec3 face_point = (vertices[v0].position + vertices[v1].position + vertices[v2].position + vertices[v3].position) *0.25f;
	face_points.push_back({face_point});
}

// edge points
std::map<std::pair<GLuint, GLuint>, int> edge_points_idx;
// This is useful for the reconstruction of new faces later.
std::vector<Vertex> edge_points;
for (const auto &edge : edges){
	GLuint v0 = edge.first;
	GLuint v1 = edge.second;
	vec3 edge_point = (vertices[v0].position + vertices[v1].position) * 0.25f;
	for (const auto &face_idx : edge_faces[edge]){
		edge_point = edge_point + face_points[face_idx].position * 0.25f;
	}
	edge_points_idx[edge] = static_cast<int>(vertices.size() + face_points.size() + edge_points.size());
	edge_points.push_back({edge_point});
}

// new vertex positions
// V' = (F + 2R + (n-3)S) / n
std::vector<Vertex> new_vertex_positions(vertices.size());
for (int i = 0; i < vertices.size(); ++i){
	vec3 F(0.0f); // average of face points
	for (const auto &face_idx : vertex_faces[i]){
		F = F + face_points[face_idx].position;
	}
	F = F * (1.0f / static_cast<float>(vertex_faces[i].size()));
	vec3 R(0.0f); // average of edge midpoints
	int edge_count = 0;
	for (int j = 0; j < edges.size(); ++j){
		if (edges[j].first == i || edges[j].second == i){
			vec3 edge_midpoint = (vertices[edges[j].first].position + vertices[edges[j].second].position) * 0.5f;
			R = R + edge_midpoint;
			edge_count++;
		}
	}
	R = R * (1.0f / static_cast<float>(edge_count));
	vec3 P = vertices[i].position; // original position
	new_vertex_positions[i].position = (F + R * 2.0f + P * (static_cast<float>(edge_count) - 3.0f)) / static_cast<float>(edge_count);
}
\end{lstlisting}
\quad Finally, we need to reconstruct the new faces using the newly computed face points, edge points, and vertex points.
There is a tricky part here is to correctly order the vertices when forming the new faces to ensure proper connectivity and orientation.
Because we need the order information to compute normals later, I carefully order the vertices in counter-clockwise order when viewed from the outside of the surface.
How I've done this? Becuase for each original face, the indices are already stored in counter-clockwise order.
So I use this information and assign vertex points and edge points accordingly.
\begin{lstlisting}
// Step 3: Reconstruct new faces
int n_vertices = vertices.size();
int n_faces = face_points.size();
int n_edges = edge_points.size();
// Combine all new points
std::vector<Vertex> new_vertices = new_vertex_positions;
new_vertices.insert(new_vertices.end(), face_points.begin(), face_points.end());
new_vertices.insert(new_vertices.end(), edge_points.begin(), edge_points.end());
std::vector<GLuint> new_indices;

for (int i = 0; i < n_faces; ++i)
{
	GLuint v0 = indices[4 * i];
	GLuint v1 = indices[4 * i + 1];
	GLuint v2 = indices[4 * i + 2];
	GLuint v3 = indices[4 * i + 3];
	// v0,v1,v2,v3 are in counter-clockwise order
	GLuint face_point_idx = n_vertices + i;
	GLuint edge_point_idx_0 = edge_points_idx[getEdgeKey(v0, v1)];
	GLuint edge_point_idx_1 = edge_points_idx[getEdgeKey(v1, v2)];
	GLuint edge_point_idx_2 = edge_points_idx[getEdgeKey(v2, v3)];
	GLuint edge_point_idx_3 = edge_points_idx[getEdgeKey(v3, v0)];
	// Therefore ep0,ep1,ep2,ep3 are also in counter-clockwise orde
	// Assign them orderly:
	new_indices.push_back(v0);
	new_indices.push_back(edge_point_idx_0);
	new_indices.push_back(face_point_idx);
	new_indices.push_back(edge_point_idx_3);

	new_indices.push_back(v1);
	new_indices.push_back(edge_point_idx_1);
	new_indices.push_back(face_point_idx);
	new_indices.push_back(edge_point_idx_0);

	new_indices.push_back(v2);
	new_indices.push_back(edge_point_idx_2);
	new_indices.push_back(face_point_idx);
	new_indices.push_back(edge_point_idx_1);

	new_indices.push_back(v3);
	new_indices.push_back(edge_point_idx_3);
	new_indices.push_back(face_point_idx);
	new_indices.push_back(edge_point_idx_2);
}
vertices = new_vertices;
indices = new_indices;
\end{lstlisting}
Therefore, we have completed one iteration of the Catmull-Clark subdivision. And at the same time, we also
be ready to compute normals for lighting calculations.\\
\quad To allow for multiple levels of subdivision, I wrapped the entire Catmull-Clark subdivision process
in a loop that runs for the specified number of levels. After completing the desired number of subdivisions,
I call the \verb|UpdateBufferData| function to update the vertex and index data on the GPU. The final effect
after several levels of subdivision is shown in Figure \ref{2}. (I forgot to save the pictures without
a lighting model, so I just use the pictures with lighting here.)
\begin{figure}[t]
	\centering
	\subfigure[level 1]{
		\label{Fig.CC_subdivision.1}
		\includegraphics[width=0.2\textwidth]{wireform_iter=1.png}
	}
	\subfigure[level 2]{
		\label{Fig.CC_subdivision.2}
		\includegraphics[width=0.2\textwidth]{wireform_iter=2.png}
	}
	\subfigure[level 3]{
		\label{Fig.CC_subdivision.3}
		\includegraphics[width=0.2\textwidth]{wireform_iter=3.png}
	}
	\subfigure[level 4]{
		\label{Fig.CC_subdivision.4}
		\includegraphics[width=0.2\textwidth]{wireform_iter=4.png}
	}
	\caption{Catmull-Clark Subdivision results at different levels.}
	\label{2}
\end{figure}

\subsection{Lighting}
\quad Before implementing lighting, I need to compute normals for the vertices of the mesh.
Normals are essential for lighting calculations as they determine how light interacts with
the surface of the object. The calculation of normals is implemented by using the cross product
of two edges of each face to get the face normal, and then averaging the face normals for each vertex.
Here is the implementation of normal calculation:
\begin{lstlisting}
void Object::calculateNormals(){
	for (auto &vertex : vertices){
		vertex.normal = vec3(0.0f);
	}
	std::vector<GLuint> tri_indices = quad_to_triangles(indices);
	for (size_t i = 0; i < tri_indices.size(); i += 3){
		GLuint i0 = tri_indices[i];
		GLuint i1 = tri_indices[i + 1];
		GLuint i2 = tri_indices[i + 2];

		Vertex &v0 = vertices[i0];
		Vertex &v1 = vertices[i1];
		Vertex &v2 = vertices[i2];
		
		vec3 edge1 = v1.position - v0.position;
		vec3 edge2 = v2.position - v0.position;
		vec3 faceNormal = glm::normalize(glm::cross(edge1, edge2));
		v0.normal += faceNormal;
		v1.normal += faceNormal;
		v2.normal += faceNormal;
	}
	for (auto &vertex : vertices){
		vertex.normal = glm::normalize(vertex.normal);
	}
}
\end{lstlisting}
\quad After calculating the normals, I implemented a simple Phong lighting model in the fragment shader.
And we only need to pass the necessary uniform variables to the shader before drawing the object.
Here is the relevant code snippet for setting up the lighting in the main loop:
\begin{lstlisting}
//...in main loop
mat4 projection = glm::perspective(radians(45.0f), static_cast<float>(WIDTH) / static_cast<float>(HEIGHT), 0.1f, 100.0f);
shader.setMat4("projection", projection);
mat4 view = mycamera.getViewMatrix();
shader.setMat4("view", view);
mat4 model = mat4(1.0f);
shader.setMat4("model", model);
vec3 lightPos = vec3(5.0f, 5.0f, 2.0f);
shader.setVec3("light_pos", lightPos);
shader.setVec3("light_ambient", vec3(0.2f, 0.2f, 0.2f));
shader.setVec3("light_diffuse", vec3(1.0f, 1.0f, 1.0f));
shader.setVec3("light_specular", vec3(1.0f, 1.0f, 1.0f));
shader.setVec3("material_ambient", vec3(0.8f, 0.8f, 0.8f));
shader.setVec3("material_diffuse", vec3(0.8f, 0.8f, 0.8f));
shader.setVec3("material_specular", vec3(1.0f, 1.0f, 1.0f));
shader.setFloat("material_shininess", 64.0f);
shader.setVec3("viewPos", mycamera.Position);
//use the draw function with shader
cube.drawElements(shader);
\end{lstlisting}
The final effect with lighting is shown in Figure \ref{3}.
\begin{figure}[t]
	\centering
	\label{Fig.Lighting}
	\includegraphics[width=0.4\textwidth]{phong_linghting.png}
	\caption{Catmull-Clark Subdivision results with a phong lighting model.}
	\label{3}
\end{figure}

\subsection{interactive UI}
\quad To allow users to interactively adjust the level of subdivision, I integrated ImGui into the application.
ImGui is a popular immediate mode GUI library that makes it easy to create user interfaces for applications.
I downloaded and set up ImGui in the project, ensuring that it works seamlessly with OpenGL and GLFW.
In the main loop, I created an ImGui window with a slider that allows users to adjust the subdivision level.
For subdivision, I save the original cube data in \verb|basic_vertices| and \verb|basic_indices|,
and when the user changes the level, the cube will be reset to the original data and then subdivided to the new level.
\begin{lstlisting}
// ImGui window for subdivision level
// In main.cpp
// Initialize ImGui
ImGui_ImplOpenGL3_NewFrame();
ImGui_ImplGlfw_NewFrame();
ImGui::NewFrame();

ImGui::Begin("Controls");
ImGui::Text("Catmull-Clark Subdivision");
ImGui::Text("Current LOD Level: %d", target_level);
ImGui::SliderInt("Level", &target_level, 0, 5);
ImGui::End();
//adjust subdivision level
if (target_level != current_level){
	cube.CatmullClarkSubdivision(target_level);
	current_level = target_level;
}
//lighting and drawing code...
// cube.drawElements(shader);
ImGui::Render();
ImGui_ImplOpenGL3_RenderDrawData(ImGui::GetDrawData());
//...
\end{lstlisting}
\quad The interactive UI allows users to easily adjust the level of subdivision and see the results in real-time.
The final effect of the interactive UI is shown in Figure \ref{4}.
\begin{figure}[t]
	\centering
	\subfigure[level 1]{
		\label{Fig.UI.1}
		\includegraphics[width=0.2\textwidth]{UI_1.png}
	}
	\subfigure[level 2]{
		\label{Fig.UI.2}
		\includegraphics[width=0.2\textwidth]{UI_2.png}
	}
	\subfigure[level 3]{
		\label{Fig.UI.3}
		\includegraphics[width=0.2\textwidth]{UI_3.png}
	}
	\subfigure[level 4]{
		\label{Fig.UI.4}
		\includegraphics[width=0.2\textwidth]{UI_4.png}
	}
	\caption{Interactive UI for adjusting subdivision level.}
	\label{4}
\end{figure}
\subsection{Adaptive Level of Detail}
\quad To implement adaptive level of detail (LOD) based on the camera's distance from the object,
I first calculated the distance between the camera and the object's center.
Based on this distance, I defined thresholds to determine the appropriate level of subdivision.
When the camera is closer to the object, a higher level of detail is used, and when the camera is farther away,
a lower level of detail is applied. The final effect of adaptive LOD is shown in Figure \ref{5}.
Here is the relevant code snippet for adaptive LOD:
\begin{lstlisting}
// In main loop
float distance_to_camera = glm::distance(mycamera.Position, cube.position);
if (distance_to_camera > 64.0f){
	target_level = 0;
}
else if (distance_to_camera > 32.0f){
	target_level = 1;
}
else if (distance_to_camera > 16.0f){
	target_level = 2;
}
else if (distance_to_camera > 8.0f){
	target_level = 3;
}
else if (distance_to_camera > 4.0f){
	target_level = 4;
}
else{
	target_level = 5;
}
\end{lstlisting}
\begin{figure}
	\centering
	\subfigure[far]{
		\label{Fig.ALOD.1}
		\includegraphics[width=0.2\textwidth]{LOD_1.png}
	}
	\subfigure[near]{
		\label{Fig.ALOD.2}
		\includegraphics[width=0.2\textwidth]{LOD_4.png}
	}
	\caption{Adaptive Level of Detail based on camera distance.}
	\label{5}
\end{figure}
\subsection{Bezier Surface Generation}
\quad This part requires us to generate a Bezier surface from a set of control points.
And this surface should be represented as a coarse quadrilateral mesh.
Firstly, we should know how to compute a point on a Bezier surface given a set of control points.
The formula for a Bezier surface is given by:
\[B(u, v) = \sum_{i=0}^{n} \sum_{j=0}^{m} P_{i,j} B_{i,n}(u) B_{j,m}(v)\]
where \(P_{i,j}\) are the control points, and \(B_{i,n}(u)\) and \(B_{j,m}(v)\) are the Bernstein polynomials
defined as:
\[B_{i,n}(u) = \binom{n}{i} u^i (1-u)^{n-i}\]
\[B_{j,m}(v) = \binom{m}{j} v^j (1-v)^{m-j}\]
To generate the Bezier surface mesh, I sampled the parameters \(u\) and \(v\) at a fixed resolution 5x5,
to create a grid of points on the surface. Here is the implementation of Bezier surface generation:
\begin{lstlisting}
int binomialCoefficient(int n, int k){
    if (k > n)
        return 0;
    if (k == 0 || k == n)
        return 1;
    k = std::min(k, n - k); // Take advantage of symmetry
    int c = 1;
    for (int i = 0; i < k; ++i){
        c = c * (n - i) / (i + 1);
    }
    return c;
}

float bernsteinPolynomial(int i, int n, float t){
    return static_cast<float>(binomialCoefficient(n, i)) * pow(t, i) * pow(1 - t, n - i);
}

void Object::generateBezierSurface(const std::vector<std::vector<vec3>> &control_points){
    basic_vertices.clear();
    basic_indices.clear();
    int n = control_points.size() - 1;    // degree in u direction
    int m = control_points[0].size() - 1; // degree in v direction
    int res = 4;

    for (int i = 0; i <= res; ++i){
        float u = static_cast<float>(i) / res;
        for (int j = 0; j <= res; ++j){
            float v = static_cast<float>(j) / res;
            vec3 point(0.0f);
            for (int k = 0; k <= n; ++k){
                for (int l = 0; l <= m; ++l){
                    float bu = bernsteinPolynomial(k, n, u);
                    float bv = bernsteinPolynomial(l, m, v);
                    point += bu * bv * control_points[k][l];
                }
            }
            basic_vertices.push_back({point});
        }
    }
    for (int i = 0; i < res; ++i){
        for (int j = 0; j < res; ++j){
            GLuint v0 = i * (res + 1) + j;
            GLuint v1 = v0 + 1;
            GLuint v2 = v0 + (res + 1) + 1;
            GLuint v3 = v0 + (res + 1);

            basic_indices.push_back(v0);
            basic_indices.push_back(v1);
            basic_indices.push_back(v2);
            basic_indices.push_back(v3);
        }
    }
    // vertices = basic_vertices;
    // indices = basic_indices;
    calculateNormals();
}
\end{lstlisting}
\quad During the subdivision of a Bezier surface, what we need to pay attention to is that
the surface is open usually, and the Catmull-Clark subdivision algorithm we implemented earlier
may introduce artifacts at the boundaries if not handled properly. To address this, I fixed the function
by adding the boundary check when building the topology information. Specifically, for new edge points
if the edge is on the boundary (i.e., it belongs to only one face), we only average the two vertex positions.
For the vertex points on the boundary (i.e., vertices connected to boundary edges), we adjust it by
averaging only the edge midpoints of the boundary edges connected to that vertex.
Here is the modified part of the Catmull-Clark subdivision function:
\begin{lstlisting}
// specail:boundary edge
if (edge_faces[edge].size() == 1){
	vec3 edge_point = (vertices[v0].position + vertices[v1].position) * 0.5f;
	edge_points_idx[edge] = static_cast<int>(vertices.size() + face_points.size() + edge_points.size());
	edge_points.push_back({edge_point});
	continue;
}
// special:boundary vertex
if (boundary_verts.count(i)){
	// Apply boundary vertex rules
	const auto &neighbors = vert_to_boundary_neighbors.at(i);
	if (neighbors.size() == 2){
		// Smooth boundary point: (1/8)P_prev + (6/8)P_curr + (1/8)P_next
		GLuint v_prev = neighbors[0];
		GLuint v_curr = i;
		GLuint v_next = neighbors[1];
		new_vertex_positions[i].position = vertices[v_prev].position * 0.125f + vertices[v_curr].position * 0.75f + vertices[v_next].position * 0.125f;
	}
	else{
		new_vertex_positions[i].position = vertices[i].position;
	}
	continue;
}
\end{lstlisting}
The final effect of the Bezier surface generation and subdivision is shown in Figure \ref{6}.
\begin{figure}[t]
	\centering
	\subfigure[coarse mesh]{
		\label{Fig.Bezier.1}
		\includegraphics[width=0.4\textwidth]{BZ_wire.png}
	}
	\subfigure[subdivison level 0]{
		\label{Fig.Bezier.2}
		\includegraphics[width=0.2\textwidth]{BZ_1.png}
	}
	\subfigure[subdivison level 1]{
		\label{Fig.Bezier.3}
		\includegraphics[width=0.2\textwidth]{BZ_2.png}
	}
	\subfigure[subdivison level 2]{
		\label{Fig.Bezier.4}
		\includegraphics[width=0.2\textwidth]{BZ_3.png}
	}
	\subfigure[subdivison level 3]{
		\label{Fig.Bezier.5}
		\includegraphics[width=0.2\textwidth]{BZ_4.png}
	}
	\caption{Bezier Surface Generation and Catmull-Clark Subdivision results at different levels.}
	\label{6}
\end{figure}

\section{Results}
\quad This assignment successfully implemented and integrated several key techniques in computer graphics and
geometric modeling. Through this practical work, the following results were achieved:
\begin{itemize}
	\item \textbf{Fundamental Mesh Construction and Rendering:} We successfully defined and rendered a cube using a quadrilateral mesh and implemented a
	      general-purpose function to convert it to a triangle mesh, establishing a solid foundation for the
	      subsequent graphics pipeline.

	\item \textbf{Catmull-Clark Subdivision:}The Catmull-Clark subdivision algorithm was fully implemented,
	      enabling the transformation of a basic cube mesh into a smooth, sphere-like surface through successive iterations.
	      As shown in Figure 2, the model's surface becomes progressively smoother and more natural as the subdivision level increases.

	\item \textbf{Realistic Rendering and Interactivity:}By implementing the Phong lighting model, we endowed the
	      geometric models with realistic shading, giving them a greater sense of depth and material presence (Figure \ref{3}).
	      The integration of an interactive UI using ImGui (Figure \ref{4}) allows the user to adjust the subdivision level in
	      real-time and instantly observe the resulting changes, significantly improving the efficiency of debugging and demonstration.

	\item \textbf{Performance Optimization and Bézier Surfaces:}An adaptive Level of Detail (LOD) system based on
	      camera distance was successfully implemented (Figure \ref{5}). This allows the application to dynamically adjust model
	      complexity, ensuring high visual fidelity for close-up views while optimizing rendering performance for distant objects.
	      Furthermore, we successfully implemented an algorithm to generate Bézier surfaces and applied a modified version of the
	      Catmull-Clark algorithm to correctly handle its open boundaries, resulting in the generation of a smooth, continuous open surface (Figure \ref{6}).
\end{itemize}
In summary, this assignment has deepened my understanding of core algorithms such as Catmull-Clark
subdivision and Bézier surface generation while also providing valuable hands-on experience in applying
theoretical knowledge to practical implementation. From low-level vertex and index buffer management to
complex topological data structures, lighting calculations, UI interactivity, and performance optimization,
each component has contributed to a more comprehensive and in-depth understanding of the modern computer
graphics pipeline. Although the implementation process was challenging, the final results are highly
satisfying and have built a strong foundation for tackling more advanced topics in computer graphics in the future.

\end{document}
